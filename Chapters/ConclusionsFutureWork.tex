\chapter{Conclusions and Future Work}
In this thesis, we have proposed formal methods and optimization techniques for the assured and efficient design of safety-critical embedded systems. We have developed a domain-specific requirements specification language, called \resa, which is tailored to embedded systems, and applied SAT-based and ontology-based techniques to improve quality of specifications. We have also proposed an approach for formally modeling hybrid Simulink models via the stochastic timed automata formalism. Our approach is based on an automated pattern-based, execution-order-preserving method to transform large-scale Simulink models into networks of stochastic timed automata. The automata are analyzed for function and end-to-end timing requirements via \uppaalsmc~\cite{Bulychev2012UPPAAL-SMC:Automata}. For resource-constrained design in general and low-battery embedded systems in particular, we have proposed efficient software allocation methods via integer linear programming and hybrid particle optimization on a network of heterogeneous computing nodes, with respect to power specification, failure rate and processor speed. During the allocation, the timing and the reliability constraints are satisfied by considering the worst-case response time, age delay, and exact reliability analysis in the context of fault tolerance ensured by node redundancy.

Our solutions are validated on various industrial automotive use cases and one benchmark. The \resa{} language is validated on the Adjustable Speed Limiter (ASL) use case, for which we are able to express around 90\% out of 300 requirements, which are mostly conditional statements. However, the 10\% of ASL requirements contain quantifiers, temporal and timed properties, which are difficult to handle via the SAT-based approach. The SAT-based approach as opposed to the ontology-based approach scales well, moreover, it is fully automated and is also integrated into the \eatop{} tool chain at VGTT. However, the analysis with the SAT-based approch is shallow due to abstraction of clauses whereas the ontology-based approach is rigorous as it operates at the lexical level but it scales less. 

Our proposed formal analysis of Simulink models is validated on the BBW system that contains 320 Simulink blocks, to which it scales well and partially on the ASL system, on which the transformation could not handle some of the Simulink blocks, hence the generated model is not immediately analyzable.  However, the limitations are due to the implementation of our tool SIMPPAAL that we also propose in this thesis, which can be improved and limitations removed by future work. 

Our proposed software allocation approaches are validated on the \autosar{} benchmark produced by Bosch, which consists of several synthetic applications. The results show that the ILP approach scales to a medium size of application software, i.e., not more than 15 software components and 60 chains. In contrast, the hybrid particle swarm optimization scales well to very large applications, consisting of 80 software components and 60 chains per application. Of the metaheuristic algorithms evaluated in our experiments, the hybrid \pso{} algorithms with hill climbing provided better quality solutions next to \ilp{} on small problems, and performed best on larger problems as compared to the classical PSO and DE algorithms, as expected. In particular the hybrid \pso{} with the stochastic hill climbing algorithm outperforms the rest.

As lines of future work, we foresee the following: (i) implementation of the ontology-based requirements analysis using open-source lexical databases, e.g., WordNet; (ii) generalization of the statistical model checking to any data-flow programming paradigms; (iii) extension of the software allocation problem to include dynamic synthesis of tasks; (iv) extension of the software allocation techniques to also address the dynamic power and dynamic configuration of software components. 
