\chapter{Conclusions and Future Work}
In this thesis, we have proposed formal methods and optimization techniques for the assured and efficient design of safety-critical embedded systems. We have developed a domain-specific requirements specification language, called \resa, which is tailored to embedded systems, and applied SAT-based and ontology-based techniques to improve quality of the specifications. We have also proposed an approach for formally analyzing hybrid Simulink models via the stochastic timed automata formalism, which is an automated pattern-based, execution-order-preserving method to transform large-scale Simulink models into networks of stochastic timed automata. The automata are analyzed for function and end-to-end timing requirements via \uppaalsmc. For resource-constrained design in general and low-battery embedded systems in particular, we have proposed efficient software allocation via integer linear programming and hybrid particle optimization on a network of heterogeneous computing nodes, with respect to power specification, failure rate and processor speed. During the allocation, the timing and the reliability constraints are satisfied by considering the worst-case response time analysis, age delay analysis, and exact reliability analysis in the context of fault tolerance.

Our solutions are validated on various industrial automotive use cases and one benchmark. The \resa{} language is validated on the Adjustable Speed Limiter (ASL) use case, that is we are able to express around 90\% of the requirements of the 300 requirements, which are mostly conditional statements. However, the 10\% of the ASL requirements contains quantifiers, temporal and timed properties, which is difficult to handle via the SAT-based approach. The SAT-based approach as opposed to the ontology-based approach scales well, moreover is fully automated and also is integrated into the ArEATOP tool chain at VGTT. However, the analysis with the former is shallow due to abstraction of clauses whereas the latter is rigorous as it operates at the lexical level albeit it scales less. 

Our proposed formal analysis of Simulink models is validated on the BBW system that contains ore than 300 Simulink blocks, to which it scales well and partially on the ASL system, on which the transformation could not handle some of the Simulink blocks, hence the generated model is not immediately analyzable.  However, the limitations are due to the implementation of our tool SIMPPAAL that we also propose in this thesis, which can be improved and limitations removed by future work. 

Our proposed software allocation approaches are validated on the \autosar{} benchmark produced by Bosch, which consists of several synthetic applications. The result showed that the ILP approach scaled to a medium size of application software, i.e., not more than 15 software components and 60 chains. Whereas the hybrid particle swarm optimization scales well to very large applications, e.g., consisting of 80 software components and 60 chains per application. Of the metaheuristic algorithms evaluated in our experiments, the hybrid \pso{} algorithms with hill climbing provided better quality solutions next to \ilp{} on small problems, and performed best on larger problems as compared to the classical PSO and DE algorithms, as expected. In particular the hybrid \pso{} with the stochastic hill climbing algorithm outperformed the rest.

Future work includes: (i) implementation of the ontology-based requirements analysis using open source lexical databases, e.g., WordNet; (ii) generalization of the statistical model checking to any data-flow programming paradigms; (iii) extend the software allocation problem to include dynamic synthesis of tasks; (iv) consider dynamic power consumption and extend the allocation to dynamic configuration of software components.
