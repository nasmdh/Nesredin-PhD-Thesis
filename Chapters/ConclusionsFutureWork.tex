\chapter{Conclusions and Future Work}
In this thesis, we proposed formal methods and optimization techniques for the assured and efficient design of safety-critical embedded systems. We developed a domain-specific requirements specification language, i.e., \resa, which is tailored to embedded systems, and applied SAT-based and ontology-based techniques to improve quality of the specifications. Furthermore, we introduced an automated pattern-based, execution order-preserving method to transform large-scale Simulink models into networks of stochastic timed automata, which is analyzed by \uppaalsmc. For resource constrained design in general and low-battery embedded systems in particular, we propose efficient software allocation via integer linear programming and hybrid particle optimization on a network of heterogeneous computing nodes, i.e., with respect to power specification, failure rate and processor speed. During the allocation, the timing and the reliability constraints are satisfied by considering the worst case response time analysis, age delay analysis, and exact reliability analysis in the context of fault tolerance.

Our solution is validated on various industrial automotive use cases and benchmark. The requirements specification language is validated on the adjustable speed limiter (\asl) use case, the statistical model checking of Simulink models on the brake-by-wire (\bbw) use case, and the software allocation on the \autosar{} Benchmark produced Bosch. Our solution is effective, and scales well to large problems considering the following summery of observations: the SAT-based approach as opposed the ontology-based approach scales well. However, the analysis of the former is shallow due to abstraction of clauses whereas the latter is rigorous as it operates at the lexical level albeit scales less. We showed analysis of hybrid Simulink models via the stochastic time automata formalism, and its scalable analysis using the statistical model checking technique, i.e., on the \bbw{} model which consists of thousands of Simulink blocks. Finally, we showed that the ILP approach to software allocation scales only to 15 software components, and the population-based metaheuristics to very large applications, e.g., with 80 software components and 60 chains per application. Of the metaheuristic algorithms the hybrid \pso{} with hill climbing provided better quality solutions next to \ilp{} on small problems and performed best on larger problems. In particular the hybrid \pso{} with the stochastic hill climbing algorithm outperformed the rest for large software allocation problems.

Future work includes: (i) implementation of the ontology-based requirements analysis using open source lexical databases, e.g., WordNet; (ii) generalization of the statistical model checking to any data-flow programming paradigms; (iii) extend the software allocation problem to include dynamic synthesis of tasks; (iv) consider dynamic power consumption and extend the allocation to dynamic configuration of software components.
