\chapter{Background}
In this section, we give the necessary background on the different concepts and technologies used in this thesis, which includes requirements boilerplates, logic-based reasoning, Simulink, stochastic timed automata, statistical model checking, integer linear programming and population-based metaheuristics. 

\section{Requirements Boilerplates}
Requirements boilerplates are frequently reoccurring patterns of statements to express software requirements. They have been first introduced to software engineering by Jeremy et al.~\cite{Hull2011RequirementsEngineering} in order to improve requirements specifications by reducing ambiguities and enabling the reuse of specification patterns. A requirement boilerplate consists of static and dynamic syntactic parts. For instance, ``\texttt{if <Condition>, <System> shall be able to <Action> within <Quantity><Unit>.}'' is a conditional boilerplate.  The syntactic elements annotated by pairs of angle brackets are dynamic (or variable), and the rest are static. The engineer has to select the right boilerplates from a repository that suits the target requirements, subsequently filling the dynamic syntactic elements to complete the specifications. 

\section{Logic-based Reasoning}
In this thesis, we express requirements specifications in  Boolean logic and apply \textit{Boolean satisfiability} (SAT) to detect inconsistencies within the specifications. Furthermore, we use \textit{ontology} to represent the specifications, and conduct rigorous analysis via a linguistic approach based on description logic. Consequently, below we briefly overview, SAT, and the notions of ontology and description logic. 

\subsection*{Boolean Satisfiability Problem}
The study of a Boolean formula is generally concerned with the set of truth assignments (assignments of 0 or 1 to each of the variables) that make the formula true. Finding such assignments by answering the simpler question: ``does there exist a truth assignment that satisfies the Boolean formula?'' is called the Boolean satisfiability problem (SAT), which is proven to be NP complete~\cite{devlin2008satisfiability}\cite{Biere2009HandbookSatisfiability}. However, efficient heuristics and approximation algorithms do exist to solve such problems. It is formally defined as follows:
\begin{definition}[SAT Problem]
Given a propositional formula $\phi = (p_1, p_2, p_3, ..., p_n)$ over the logical connectives $\neg,\lor,\land,\rightarrow$, where: $p_1, ..., p_n$ are atomic propositions, the SAT problem equates to checking if there exists an interpretation (or a model) that satisfies $\phi$, that is, a TRUE or FALSE assignment to the variables of $p_1, ..., p_n$, such that $\phi$ evaluates to true, in which case $\phi$ is called satisfiable. In the opposite case, if no such assignment exists, then $\phi$ is unsatisfiable~\cite{devlin2008satisfiability}.  
\end{definition}

SAT problems are usually checked using decision procedures known as SAT solvers. The Z3 tool integrates several background theories (or Satisfiability Module Theories, SMT) and decision procedures based on SAT solvers~\cite{DeMoura2008Z3:Solver}. It is a well known SMT solver and a theorem prover developed by Microsoft Research that has been used in the formal verification and reasoning of software and hardware systems, mainly due to its strong support for integration to verification tools via its well known APIs written, e.g. in Java, Python, C.

The input language of Z3 is an extension of the SMT-LIB2 standard, consisting of commands used to assert logical statements and instruct the solver to do some action, e.g., to check for satisfiability, in which case the command \textit{(check-sat)} is invoked. Z3 returns \textit{sat} if the formulas are satisfiable. If the formulas are not satisfiable, Z3 returns \textit{unsat}, or if it cannot decide the result, it returns \textit{unknown}. In case of unsat, if the unsat-core option is enabled, Z3 can return the subset of unsatisfiable assertions by invoking the \textit{(get-unsat-core)} command.

\subsection*{Ontology and Description Logic}
\textit{Ontology} is a knowledge representation technique frequently used in the artificial intelligence and other domains to facilitate automated decision making. It is defined as ``a formal specification of a shared conceptualization'' according to Borst \cite{Borst1997ConstructionOntologies}. It employs different modeling entities, e.g., concepts, instances, axioms, which are compared to classes, objects, inheritance in Object-Oriented Programming. The ontology is consistent if all axioms and relations (or assertions) hold~\cite{Mankovskii2009OWL:Language}.

\textit{Description logic} (DL)~\cite{Baader2010TheApplications} is a knowledge presentation language, and is the most widely-used language to construct ontologies. It is a fragment of first-order logic with many of the core reasoning problems decidable. For this reason, it has several applications, e.g., in web semantics (e.g., OWL)~\cite{conf/owled/ShearerMH08}, artificial intelligence~\cite{10.1007/978-94-017-9297-4_7}, bioinformatics~\cite{Rector2006}, software engineering, natural language processing. Hence, it is usually backed by solid reasoning services that terminate and deliver results in reasonable time. 

\section{Simulink}
Simulink~\cite{JamesB.Dabney2003MasteringSimulink} is a graphical development environment for the modeling, simulation and analysis of embedded systems, which is widely used in industry to model and inspect the dynamics of systems before implementation. It is robust as it supports multi-domain, continuous, discrete, hybrid systems, also discrete systems that execute with different sampling times (or multi-rate). Figure~\ref{fig_sm_multi-rate} shows a multi-rate subsystem of the Brake-by-Wire Simulink model that models the brake pedal (that is continuous behavior) and global brake functionality (that is, discrete behavior), which execute every 10ms and 20ms.  
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{images/sm}
	\caption{Brake pedal and global brake controller of the Brake-by-Wire model.}
	\label{fig_sm_multi-rate}
\end{figure}

Simulink is frequently used in the development of safety-critical systems, from the automotive, avionics or industrial automation systems domains, being often used to generate code automatically from the models. Therefore, it is crucial that such models are analyzed rigorously. The Simulink~Design~Verifier (SDV)~\cite{MathWokrksSimulinkVerifier} provides a formal verification technique that is based on the exact model checking to exhaustively verify low-level properties such as buffer overflow, division by zero etc. However, the tool's functionality is limited as it lacks support for the analysis of high-level properties, such as invariance, reachability, timing, which is vital to ensure predictability of safety-critical embedded systems. 

\subsection*{Simulink Blocks}
A Simulink model is constructed from communicating function \textit{blocks} (or Simulink blocks). The blocks implement simple to complex functionality and consist of Input and Output ports, which enable communication via connectors. The modeling elements support basic and user-defined datatypes, e.g., integer, floating-point, and simple and complex data structures, namely scalar, vector, matrix.  Simulink blocks are classified into \textit{virtual} (non-computational, e.g., Mux, Demux blocks) and \textit{non-virtual} (computational, e.g., Gain, Integrator) blocks based on their computability. The non-virtual blocks improve the visualization of the model, but unlike the non-virtual blocks, they are not executed, hence do not affect the execution semantics of the model.  The blocks can be composed into groups using Subsystem, Model blocks, to enable hierarchical modeling, which is used to improve the visualization, and to enforce execution order on a group of blocks. The fundamental constructs of the composite blocks are \textit{atomic} blocks, like Gain, Sum blocks.

The non-virtual (computational) atomic blocks can be categorized into \textit{continuous} and \textit{discrete} blocks based on the execution semantics of the blocks.  A discrete block executes periodically with sample time $t_s$, whereas a continuous block executes over infinitesimal sample times. Since the Simulink blocks libraries are not usually sufficient to model practical embedded systems, the framework supports mechanisms to extend functionality, which engineers can exploit to develop complex systems. The mechanisms include S-function, Custom Block and Masking. S-function is a computer language of Simulink blocks which allows advanced implementations of block routines, written in MATLAB, C, C++, or Fortran.


\subsection*{Execution of Simulink Blocks}
During the initial phase of the simulation in the Simulink environment, the model is compiled, thus the order in which the blocks are executed is established in the \textit{sorted order} list (slist). Figure~\ref{fig_sm_exec_order} shows the execution order via the labels in red color, annotated as $s:b$, where $s$ denotes the system/subsystem index and $b$ the block index\footnote{https://se.mathworks.com/help/simulink/ug/controlling-and-displaying-the-sorted-order.html}. Basically, the list is determined according to the data dependency of block outputs on the block inputs ports, that is, if the output depends on the current value of the input, the input port is identified as \textit{direct-feedthrough} port. Thus, to preserve the data dependency in the model, the sort-order rules require that the blocks that derive other blocks with direct-feedthrough ports must come first in the list, e.g., blocks that derive Gain block. However, the blocks with non-direct-feedthrough ports, e.g., the Delay block in Figure~\ref{fig_sm_exec_order}, can execute in any order, considering that the previous rule applies. The execution order is also affected by the user-defined priorities, nevertheless, the priorities do not violate the rules. The sorted order list can be fetched by simulating the model in the debugging mode.
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{images/sm_exec_order}
	\caption{The Brake-by-Wire Subsystem block that computes vehicle speed. The labels in red color denote of the execution order of the blocks.}
	\label{fig_sm_exec_order}
\end{figure}

\section{Stochastic Timed Automata}
The theory of \textit{stochastic timed automata}\cite{David2011StatisticalAutomata} builds upon the timed automata theory and adds probabilistic distributions to support stochastic behavior in modeling and analysis of real-time systems. 

A timed automaton is a finite-state automaton proposed by Alur et al.~\cite{Alur1999TimedAutomata} to model and analyze real-time systems, by extending finite-state automata with real-valued clock variables. A timed automaton (TA) $\mathcal{A}$ is defined as a tuple $\langle L,l_0,X,\Sigma,E,I\rangle$, and its elements are described as follows: $L$ is a set of locations of which $l_0$ is the initial location, $X$ is the set of clocks, $E$ is the set of edges between locations,  $\Sigma$ is a set of action labels, and $I$ is a set of invariants, which are Boolean constraints over $X$ assigned to locations, that is, the automaton can only stay in the location as long as its invariant holds. An edge $(l,g,a,r,l')\in E$ denotes a transition relation from location $l$ to $l'$, with guard $g$, action $a$ and clock resets $r$. The guard is conjunction of clock constraints, and when it holds, the underlying transition corresponding to traversing the edge is fired.


A stochastic timed automaton (STA), as used in \uppaal{} Statistical Model Checker (\uppaalsmc) is defined by the tuple: $STA =\langle\mathcal{A},\mu,\gamma\rangle$, where A is a timed automaton, $\mu$ is the set of all density delay functions, given by either a uniform (for bounded delays via location invariants) or an exponential distribution (for unbounded delays), and $\gamma$ is the set of all output probability functions over the output edges of the automaton. 
\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{images/sta_example}
	\caption{STA example.}
	\label{fig_staexample}
\end{figure}

Assume that, $d(l,v)$ is the infimum delay that satisfies the disjunction of guards, and assume that, $D(l,v)$ is the supremum delay before enabling and output. If the delay is bounded, i.e., there exists $D(l,v)$, in location $l$, each delay density function in $l$, $\mu_s$, is assumed to be uniformly distributed in the interval $[d(l,v),D(l,v)]$. However, if the location is time unbounded, the delays are assumed to be exponentially distributed with the rate $R(l)$. For more details on STA, we refer the reader to the literature~\cite{David2011StochasticAutomata}.

\begin{example}[STA Example] Figure~\ref{fig_staexample} is an artificial STA example with three locations A, B and C, where A is the initial location. The guards of the automaton are colored in green (e.g., $x\geq10$ over the edge that connects A and B), the clock resets are colored in blue (e.g., $x=2$ on the same edge), and the location invariants are colored in purple (e.g., $x\leq 10$ on location C). The number in red on location A is the rate of exponential.
The rate of exponential is in fact a user-defined value for the distribution parameter $\lambda$ in the delay function that calculates the probability that the automaton leaves the specified location at each simulation step, as follows:
$Pr(\mbox{leaving location A after } t) = 1 - e^{(-\lambda t)}  = 1000$. Basically, the greater the value of $\lambda$, the higher the probability that the automaton leaves the location.
\end{example}

In the example of Figure~\ref{fig_staexample}, the delays at the location A are exponentially distributed, i.e., with rate 1000, since the location is time unbounded, and delays at B and C are uniformly distributed in the interval $[4,8]$ and $[2,10]$, respectively, since the delays are timed bounded. The output probability at locations A and C is 1 and at location B is 1/2.
%Note: logical path, and probability of taking the path is purely stochastic

\subsection*{Network of Stochastic Timed Automata}
Under the assumption of input-enabledness, disjointness of clock sets and output action sets, a \textit{network of STA} (denoted by NSTA) is parallel composition of $STA_i$~\cite{David2011StochasticAutomata}, that is $(STA_1||STA_2||,...,||STA_n)$, where $n$ is the number of STA in the network. The state of an NSTA is a tuple $\langle s_1,s_2,...,s_n\rangle$, where $s_i = (l_i, v_i)$, where $l_i$ is a location of $STA_i$, and $v_i$ is the valuation of clocks $X_i$. In NSTA, the automata communicate through broadcast channels and globally shared variables. Moreover, the semantics of the NSTA relies on the independent computation of the individual STA (or components), i.e.,  each component automaton, based on the delay density functions and the output probability functions decides repeatedly on which output to generate and at which point in time. In this race, the output will be determined by the component automaton that has chosen to produce the output after the minimum delay~\cite{David2011StatisticalAutomata}.


\section{Statistical Model Checking}
\textit{Statistical model checking}~\cite{Agha2018AChecking} uses a finite set of randomly selected execution traces (or runs) of the state-based system $S$ to check if the traces provide probability evidence for accepting or rejecting the specification (or property) $\varphi$, i.e., $T\in Traces(S)\models \varphi$. It applies statistical techniques such as hypothesis testing and Monte Carlo simulation to compute the probability evidence. Since exhaustive model checking encounters the state-space explosion problem for large models, statistical model checking is a viable alternative to exhaustive verification via probability estimation. 

UPPAAL Statistical Model Checker (\uppaalsmc)~\cite{Bulychev2012UPPAAL-SMC:Automata} is an extension of the model checker \uppaal{} with support for statistical model checking. It admits a system modeled as NSTA, and the properties to checked are specified in probabilistic weighted metric temporal logic (PWMTL)~\cite{David2011StatisticalAutomata}, which is the probabilistic extension of weighted metric temporal logic (WMTL). The properties are of the following form: 
\begin{align}\label{eqn_wmtl}
\varphi::=P(\Diamond_{x\leq T}\phi)\bowtie p | P(\Box_{x\leq T})\bowtie p
\end{align}
where $x$ is a clock, $T\in \mathbb{N}$ is a bound, $\phi$ is a state predicate, $\bowtie \in\{<,\leq,=,>,\geq\}$, and $p \in [0, 1]$.

\uppaalsmc~\cite{David2011TimeSystems}\cite{Bulychev2012UPPAAL-SMC:Automata} can be used to check qualitative properties such as hypothesis testing and probability comparison, as well as quantitative ones via probability estimation. The property $\varphi$ of Equation (\ref{eqn_wmtl}) shows the definition of hypothesis testing. The probability of making hypothesis testing errors is bounded by the strength parameters $\alpha$ and $\beta$, where $\alpha$ is the probability of accepting the null hypothesis while its opposite holds (false negative), and $\beta$ is the probability of accepting the opposite of the null hypothesis while the null hypothesis holds (false positive) and $p$ is the estimated probability of the property holding. Another qualitative property that can be checked using \uppaalsmc{} is the comparison of probabilities.

The probability evaluation is a quantitative method that determines the probability of satisfying a PWMTL property $\varphi$ of an STA. The evaluation is an approximation interval $[p-\mu,p+\mu]$ with a confidence $1-\alpha$, where $\mu$ and $\alpha$ are user configurable parameters. The number of traces is determined automatically by the model checker, based on the parameters. 

\section{Integer linear Programming}
An integer linear programming (ILP)~\cite{Bradley1977AppliedProgramming} is a type of optimization problem that consists of integer decision variables, and the linear objective function and constraints. The binary ILP problem is a special case of ILP where decision variables are binary. It is represented as:
\begin{align}
    \label{eqn_ilp_obj}
	&Maximize\ \sum_{j=1}^{n}{c_jx_j}\\\nonumber
	&\mbox{Subject to:}&\\
	\label{eqn_ilp_cons}
	&\sum_{j=1}^n{a_{ij}x_j}\leq b_i&\mbox{ for all } i=1,...,m\\
	\label{eqn_ilp_bdv}
	&x_j\in\{0,1\} &\mbox{ for all } j=1,...,n
\end{align}
where (\ref{eqn_ilp_obj}) is the objective function with $n$ cost coefficients $c_j:j=1,...,n$, (\ref{eqn_ilp_cons}) are $m$ linear constraints with coefficients $a_j:1,...,m$ and bounds $b_j:1,...,m$, and (\ref{eqn_ilp_bdv}) are binary decision variables with $x_j:1,...,n$.

ILP is used in many control and engineering applications, e.g., fleet and flight routing in transportation, airfoil design in avionics engineering, controller design in industrial automation, automotive, etc., resource allocation. The binary variant is applied in several resource allocation problems, like assignment of operating system tasks to processors. The software allocation, for instance, in component-based development, can be formulated as a binary integer programming problem, where $x_j=1$ denotes the fact that a component is mapped to the $j^{th}$ processor and $x_j=0$ denotes the fact that a component is not mapped to the $j^{th}$ processor.

ILP problems are frequently solved via exact algorithms, e.g., branch and bound, but can also be dealt with heuristics, e.g., using simulated annealing, hill climbing. In this work, we use the CPLEX solver~\footnote{https://www.ibm.com/analytics/cplex-optimizer} form IBM to the software allocation optimization.'

\section{Population-based Metaheuristics}
The ILP approach and for that matter exact methods do not scale well on complex optimization problems, or else their application is usually prohibitively expensive when applied on large-scale problems, i.e., with many decision variables. In contrast, metaheuristics~\cite{2006HandbookMetaheuristics}\cite{Gonzalez2007HandbookMetaheuristics}, which is a type of heuristics with search strategy, is more efficient computation-wise, albeit less optimal. The population-based metaheuristics is a class of meta-heuristic algorithms which uses a set of individuals (or population) to guide the search strategy and determine the global optima of the problem. The most common population-based algorithms consist of evolutionary algorithms, differential evolution~\cite{Storn1997DifferentialSpaces}\cite{Das2016RecentSurvey}, particle swarm optimization~\cite{Poli2008AnApplications}\cite{Mirjalili2019ParticleOptimisation}.

In this thesis work, for the efficient software allocation, we apply the differential evolution, particle swarm optimization (PSO) and the latter's hybrid with local search algorithms such as hill climbing and stochastic hill climbing to find the best (or near optimal) solution. In the case of differential evolution, special evolutionary operators, such as mutation, crossover and selection, are employed to update the population, thus traverse across the search space to reach at the global near-optima. The technique is briefly described as follows. A mutant agent (or individual), i.e., the offspring $\textbf{v}$, is generated for every agent from three other agents in each iteration according to (\ref{eqn_de_mutation}), subsequently combined using the crossover operator (\ref{eqn_de_crossover}). If the offspring $\textbf{u}$ is more fit than the agent $\textbf{x}$, i.e., closer to the best solution, it replaces the agent otherwise is discarded as shown in the selection strategy (\ref{eqn_de_selection}).
\begin{align}
\label{eqn_de_mutation}
\textbf{v} & \leftarrow   \textbf{a} + F\circ(\textbf{b}-\textbf{c})\\
\label{eqn_de_crossover}
\textbf{u}&\leftarrow crossOver(\textbf{v},\textbf{x},CF,F)\\
\label{eqn_de_selection}
\textbf{x} &\leftarrow 
\begin{cases}
	\textbf{u} & \mbox{if } f(\textbf{u}) < f(\textbf{x})\mbox{ functions}\\
	\textbf{x} & \mbox{otherwise }
\end{cases}
\end{align}
where $F\in[0,2]$ is known as differential weight and controls the diversity of the mutant, and $CF\in[0,1]$ is known as the crossover probability and controls the degree of crossover between the agent and the mutant.

Similarly, the particle-swarm optimization employs individuals (or particles in this case). The particles unlike in the differential evolution are memory-based, i.e., they record their best position so far $\textbf{p}_{bst}$, and the algorithm remembers the best position of the population $\textbf{z}$. The motion of each particle is guided by its velocity (\ref{eqn_pso_velocity}) and attractions towards its best position $\textbf{p}_{bst}-\textbf{p}$ and towards the best position of the population $\textbf{z}-\textbf{p}$ (\ref{eqn_pso_position}), where $\textbf{p}$ is an n-dimensional matrix which represents the current position of a particle.
\begin{align}
\label{eqn_pso_velocity}
\textbf{v} &\leftarrow  \omega\textbf{v} + c_1Rand()\circ(\textbf{p}_{bst}-\textbf{p}) + c_2Rand()\circ(\textbf{z}-\textbf{p})\\
\label{eqn_pso_position}
\textbf{p} &\leftarrow \textbf{p} + \textbf{v},
\end{align}
where $\omega$ is the weight of the velocity, also known as \textit{inertia coefficient} and controls the convergence of the algorithm. The $c_1, c_2$ constants are acceleration coefficients and control the weight of attraction towards the cognitive and social components, respectively. $Rand()\in U(0,1)$ is a random function along the acceleration coefficients, which is element-wise multiplied with the components to improve diversity of the search by introducing stochastic behavior