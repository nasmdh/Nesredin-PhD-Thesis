\chapter{Related Work}
In this section are discussed the related work on specification and analysis of requirements, formal analysis Simulink models and allocation of software applications,

\section{Specification and Analysis of Embedded Systems Requirements}
Embedded system requirements are captured in different representations, e.g., textual, tabular, graphical, etc. The textual representation, which is the scope of the analysis, can be conveniently classified into two classes: i) controlled natural language (CNL), ii) template-based methods.  The syntax and semantics of the CNLs are similar to natural language except that the lexicon and the syntax are restricted for different reasons, of which improving comprehensibility of the text and formal representations to support rigorous analysis of the text are prominent. The \resa{} language is designed to improve comprehensibility of requirements specifications as well as to be computer-processable. There are many computer-processable CNL in literature \cite{Kuhn2014ALanguages}, e.g., Attempto Controlled English (ACE) \cite{attempto96}, Processable English (PENG) \cite{Schwitter2002EnglishLanguage}, etc. Similar to most computer processable languages, \resa{} has limited syntactic constructions, and allows knowledge representations, in contrast though, our language caters for embedded systems and therefore it uses concepts as well as semantic rules that are domain-specific to embedded systems. Similar to PENG, its implementation supports look-ahead in order to enable predictive and guided specification.

The template-based methods, in particular requirements boilerplate uses templates (or boilerplates), which are reusable, recurrent patterns to specify requirements, e.g., CESAR boilerplates \cite{Farfeleder2011DODT:Development}, RAT, etc. The main drawback of existing requirements boilerplates are: i) the templates are usually too limited therefore not expressive enough, ii) it is not easy to find the appropriate boilerplate during specification. In this regard, \resa{} extends boilerplates with a meta-model that guides a plausible instantiation of boilerplates.


\section{Formal Analysis of Simulink Models}
Several research endeavors have tackled the problem of formally analyzing Simulink models in order to gain better insight into the design of Simulink models, and they mainly differ in their coverage of the Simulink language, their robustness to formally analyze complex Simulink models, which is crucial for applicability of the proposed methods in industry. In this related work, we focus on techniques that employ model checking due to extra benefit of automation, such as over theorem proving, SAT solving.% on formal analysis of Simulink models, existing formalisms and their applicability in industry are discussed, and also compared to our solutions.

Simulink already supports formal analysis of Simulink models via its Simulink Design Verifier (SDV) product\footnote{https://se.mathworks.com/products/sldesignverifier.html}. Although there are limited resources that investigate the pros and cons of the product, the study by Nellen et al. \cite{Nellen2018FormalRecommendations} and Florian et al.~\cite{Leitner2008SimulinkStudy} indicates some limitation of the product, e.g., its inability to specify timed properties (e.g., cause-response LTL properties), degraded performance and less scalability against the SPIN model checker, inability to detect problems caused by race conditions in concurrent processes. 

The PlasmaLab proposed by Axel Legay et al.~\cite{Legay2016StatisticalLab} transforms Simulink sample traces into statistical models, which are eventually analyzed by their statistical mode checker. Although the checker is assisted by an algorithm that determines sufficiency of the sample traces, it is unclear how it works. Unlike many approaches, PlasmaLab can analyze any Simulink models as long as the simulation results sample trances, which is its main advantage. %Ferrante et al. \cite{Hocking2016ProvingModels} use contract-based theory in order to lift the block specification, and rely on a combination of SAT solvers and the NuSMV model checker for analysis. Hocking et al. \cite{Ferrante2012ParallelSystems} use the PVS specification language for writing the specification, and rely on the PVS theorem prover for analysis. A limitation of this strategy is that both steps still require much user interaction, so it is error-prone and requires certain understanding of the formal analysis engines, which is not common among embedded systems engineers.

%The model-to-model transformation approach basically refers to the transformation of the Simulink model into a formal model that can be checked via model checking, e.g., for reachability properties.
Other related work relay on exact model checking as opposed to statistical, e.g., Meenakshi et al. \cite{Meenakshi2006ToolChecker} propose a transformation into the input language of NuSMV model checker and supports only discrete Simulink blocks. Similarly, Bernat et al.~\cite{Barnat2012ToolDesigns} propose a transformation into an intermediate language, followed by compilation of the latter into Common Explicit-State Model Interface (CESMI) specification, which is an input language to the \devine{} model checker. These related work are limited in scalability due the use of exact model checking, thus cannot be use on complex and large-scale Simulink models. Some research endeavors propose transformations of just StateFlow/Simulink into Baysian statistical model~\cite{Zuliani2010BayesianVerification}, hybrid automata~\cite{ManamcheriSukumar2011TranslationAutomata}, the input language of \uppaal{}~\cite{Jiang2016FromDesign}. 

In contrast to the discussed related work, our approach is distinct mainly in three ways: i) we transform the Simulink model into networks of stochastic timed automata, which is checked via the statistical model checking, as opposed to exact model checking, using \uppaalsmc; ii) we verify the routines of the Simulink blocks, which are implemented in C, using Dafny (a program verifier) before the transformation; iii) our approach supports transformations of any type of Simulink blocks, and discrete and continuous Simulink models.


\section{Software Allocation Optimization}
Different allocation schemes deliver different system performance and therefore efficient software allocation is crucial. Ernest Wozniak et al. \cite{Wozniak2013AnArchitectures} proposed a synthesis mechanism for an AUTOSAR software application that can execute over multiple nodes, with the objective of fulfilling timing requirements. In contrast, we consider power consumption and reliability requirements besides timing. Similarly, Salah Saidi et al. \cite{Saidi2015AnArchitectures} proposed an ILP based approach for allocation of an AUTOSAR application on a multi-core framework in order to reduce the overhead of inter-process communication while we consider a network of computing nodes. Ivan Svogor et al. \cite{vsvogor2014extended} proposed a generic approach of identifying resource constraints and a way of handling different measurement units with Analytic Hierarchy Process (AHP) in order to allocate a component-based software application on a heterogeneous platform. However, the resource constraints are trivialized, e.g., end-to-end delay calculations, which require complex timing analysis especially if data age constraints are considered~\cite{mubeen2013support}. In addition AHP scales for at most 10 components. As opposed to the previously mentioned related work, we considered a system model with multi-rate tasks, and data age constraints on the end-to-end communication of the tasks. Moreover, we consider fault-tolerant application, i.e. by replicating software components, to maximize the reliability of the application, hence meeting reliability requirements, that is in parallel to satisfying the end-to-end timing requirements. On a different front, there exists research focusing on power and energy consumption in real-time distributed systems which consider dynamic voltage scaling ~\cite{bambagini2016energy} and task consolidation to minimizing computational nodes ~\cite{faragardi2013towards}~\cite{devadas2012interplay}. However, the former is applicable after software integration, that is during runtime, and the latter do not consider power consumption minimization as their objective function, though target reducing the number of computing nodes.