\chapter{Related Work}
In this section, we discuss and compare to the related work on specification and analysis of requirements, formal analysis Simulink models and allocation of software applications.

\section{Specification and Analysis of Embedded Systems Requirements}
Embedded system requirements are captured in different representations, e.g., textual, tabular, graphical, etc. The textual representation can be conveniently classified into two classes: i) controlled natural language (CNL), ii) template-based methods.  The syntax and semantics of the CNL are similar to natural language except that the lexicon and the syntax are restricted for different reasons, of which improving comprehensibility of the text and formal representations to support rigorous analysis of the text are prominent. The \resa{} language is designed to improve comprehensibility of requirements specifications as well as to be computer-processable. There are many computer-processable CNL in literature \cite{Kuhn2014ALanguages}, e.g., Attempto Controlled English (ACE) \cite{attempto96}, Processable English (PENG) \cite{Schwitter2002EnglishLanguage}, etc. Similar to most computer-processable languages, \resa{} has limited syntactic constructions, and allows knowledge representation. However, unlike Attempto or PENG, it is tailored to the specification of embedded systems requirements.  On the cons side, it does not support advanced natural language processing features, like anaphra parsing. Similar to PENG, its implementation supports ''look-ahead`` in order to enable predictive and guided specification.

The template-based methods, in particular requirements boilerplate uses templates (boilerplates), which are reusable, recurrent patterns to specify requirements, e.g., CESAR boilerplates \cite{Farfeleder2011DODT:Development}. The main drawback of existing requirements boilerplates are: i) the templates are usually too limited therefore not expressive enough, and ii) it is not easy to find the appropriate boilerplate during specification. In this regard, \resa{} extends boilerplates with a meta-model that guides a plausible instantiation of boilerplates.


\section{Formal Analysis of Simulink Models}
Several research endeavors have tackled the problem of formally analyzing Simulink models in order to gain better insight into the design of Simulink models. The approaches differ mainly in their coverage of the Simulink language, their robustness to formally analyze complex Simulink models, which is crucial for applicability of the proposed methods in industry. In this related work, we focus on techniques that employ model checking due to the extra benefit of a higher degree of automation if compared to theorem proving methods. .% on formal analysis of Simulink models, existing formalisms and their applicability in industry are discussed, and also compared to our solutions.

Simulink already supports the formal analysis of descriptions via its Simulink Design Verifier (SDV) product\footnote{https://se.mathworks.com/products/sldesignverifier.html}. Although there are limited resources that investigate the pros and cons of the product, the studies by Nellen et al. \cite{Nellen2018FormalRecommendations} and Florian et al.~\cite{Leitner2008SimulinkStudy} indicate some limitation of the product, e.g., its inability to specify timed properties (e.g., cause-response LTL properties), degraded performance and less scalability against the SPIN model checker, inability to detect problems caused by race conditions in concurrent processes. 

The PlasmaLab proposed by Axel Legay et al.~\cite{Legay2016StatisticalLab} transforms Simulink sample traces into statistical models, which are eventually analyzed by their statistical model checker. Although the checker is assisted by an algorithm that determines sufficiency of the sample traces, it is unclear how it works. Unlike many approaches, PlasmaLab can analyze any Simulink models as long as the simulation delivers sample traces, which is the main advantage of the tool.  %Ferrante et al. \cite{Hocking2016ProvingModels} use contract-based theory in order to lift the block specification, and rely on a combination of SAT solvers and the NuSMV model checker for analysis. Hocking et al. \cite{Ferrante2012ParallelSystems} use the PVS specification language for writing the specification, and rely on the PVS theorem prover for analysis. A limitation of this strategy is that both steps still require much user interaction, so it is error-prone and requires certain understanding of the formal analysis engines, which is not common among embedded systems engineers.

%The model-to-model transformation approach basically refers to the transformation of the Simulink model into a formal model that can be checked via model checking, e.g., for reachability properties.
Other related work relies on exact model checking as opposed to statistical, e.g., Meenakshi et al. \cite{Meenakshi2006ToolChecker} propose a transformation into the input language of NuSMV model checker and supports only discrete Simulink blocks. Similarly, Bernat et al.~\cite{Barnat2012ToolDesigns} propose a transformation into an intermediate language, followed by compilation of the latter into Common Explicit-State Model Interface (CESMI) specification, which is an input language to the \devine{} model checker. These related approaches are limited in scalability due the use of exact model checking, thus cannot be used on complex and large-scale Simulink models. Other research endeavors propose the transformations of only StateFlow/Simulink models into Bayesian statistical models~\cite{Zuliani2010BayesianVerification}, hybrid automata~\cite{ManamcheriSukumar2011TranslationAutomata}, timed automata~\cite{Jiang2016FromDesign}.

In contrast to the discussed related work, our approach is distinct mainly in three ways: i) we transform the Simulink model into NSTA, which are checked via statistical model checking, using \uppaalsmc, which tames scalability with respect to large industrial Simulink models; ii) we verify the routines of the Simulink blocks, which are implemented in C, using Dafny (a program verifier developed at Microsoft Research); iii) our approach supports transformations of any type of Simulink blocks, thus it can handle mixed discrete-time and continuous-time Simulink models. 


\section{Software Allocation Optimization}
Different allocation schemes deliver different system performance and therefore efficient software allocation is crucial. Ernest Wozniak et al. \cite{Wozniak2013AnArchitectures} proposed a synthesis mechanism for an AUTOSAR software application that can execute over multiple nodes, with the objective of fulfilling timing requirements. In contrast, we consider power consumption and reliability requirements besides timing. Similarly, Salah Saidi et al. \cite{Saidi2015AnArchitectures} proposed an ILP based approach for allocation of an AUTOSAR application on a multi-core framework in order to reduce the overhead of inter-process communication while we consider a network of computing nodes. Ivan Svogor et al. \cite{vsvogor2014extended} proposed a generic approach of identifying resource constraints and a way of handling different measurement units with Analytic Hierarchy Process (AHP) in order to allocate a component-based software application on a heterogeneous platform. However, the resource constraints are trivialized, e.g., end-to-end delay calculations, which require complex timing analysis especially if data age constraints are considered~\cite{mubeen2013support}. In addition AHP scales for at most 10 components. As opposed to the previously mentioned related work, we considered a system model with multi-rate tasks, and data age constraints on the end-to-end communication of the tasks. Moreover, we consider fault-tolerant application, i.e. by replicating software components, to maximize the reliability of the application.

Consequently, our allocation approach considers both reliability requirements as well as end-to-end timing ones as constraints to be satisfied.  On a different front, there exists research focusing on energy consumption in real-time distributed systems which consider dynamic voltage scaling~\cite{bambagini2016energy}, which is appliable during runtime  but does not consider reliability and end-to-end timing, and other works, like task consolidation to minimizing computational nodes~\cite{faragardi2013towards}\cite{devadas2012interplay}, which do not consider power consumption as the objective function.